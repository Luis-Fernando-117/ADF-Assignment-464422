---
title: "ADF Individual Assignment Report"
author: "Luis F. Barros - 464422"
date: "Last compiled on `r Sys.Date()`"
output: 
    bookdown::html_document2:
        toc: true
        toc_float: 
            collapsed: false
        number_sections: true
        code_folding: hide
        theme: cerulean
editor_options: 
  chunk_output_type: console
---

For this notebook, it is needed to load an R.Data file containing all the raw 
information from Twitter that I got from "collect_data.R" script. 
The output should be another .RData file with the preprocessed data that has the data converted
into data frames ready to analyze.

search_tweets = returns twitter statuses from tweets matching certian words or strings of the past 6-9 days 

SearchTwitter = gives you tweets that contain certain words or phrases

Documentation of search_tweets
#https://www.rdocumentation.org/packages/rtweet/versions/0.7.0/topics/search_tweets

Documentation of search_twitter
https://www.rdocumentation.org/packages/twitteR/versions/1.1.9/topics/searchTwitter

Load Libraries 
```{r}
library("httr")
library("jsonlite")
library("tidyverse")
library("twitteR")
library("rtweet")
library("plyr")
library("dplyr")
library("data.table")
```

Load Objects
```{r}
# Followers tweets
  load("text.tweets.obrador.followers.RData")
  load("text.tweets.pinera.followers.RData")
  load("text.tweets.lenin.followers.RData")
  load("text.tweets.bukele.followers.RData")
```

Create a tibble with the words and their count 

```{r}
  word.count.obrador.followers <- text.tweets.obrador.followers %>% unnest_tokens(word, text)
  word.count.pinera.followers  <- text.tweets.pinera.followers  %>% unnest_tokens(word, text)
  word.count.lenin.followers   <- text.tweets.lenin.followers   %>% unnest_tokens(word, text)
  word.count.bukele.followers  <- text.tweets.bukele.followers  %>% unnest_tokens(word, text)
```



```{r}
# Get some features out of the tmls data frame
average_text_with <- mean(tmls$display_text_width)
average_retweet_count <- mean(tmls$retweet_count)

```

